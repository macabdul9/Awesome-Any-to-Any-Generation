# Awesome-Any-to-Any-Generation
This repository is dedicated to showcasing state-of-the-art techniques and implementations of any-to-any generative models, which are capable of generating any modality (audio/speech, image/vision, text) from any modality (audio/speech, image/vision, text) as input.

**Models**
1. AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling [Paper](https://arxiv.org/abs/2402.12226)
2. CoDI: Any-to-Any Generation via Composable Diffusion [Paper](https://arxiv.org/abs/2305.11846)
3. CoDi2: In-Context, Interleaved, and Interactive Any-to-Any Generation [Paper](https://arxiv.org/abs/2311.18775)
4. C3Net: Compound Conditioned ControlNet for Multimodal Content Generation [Paper](
5. C3LLM: Conditional Multimodal Content Generation Using Large Language Models [Paper](https://arxiv.org/abs/2405.16136)
6. Visual Echoes: A Simple Unified Transformer for Audio-Visual Generation [Paper](https://arxiv.org/abs/2405.14598)
7. NExT-GPT: Any-to-Any Multimodal LLM [Paper](https://arxiv.org/abs/2309.05519)
8. Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks [Paper](https://arxiv.org/abs/2206.08916)
9. Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action [Paper](https://arxiv.org/abs/2312.17172)
10. C3Net: Compound Conditioned ControlNet for Multimodal Content Generation
11. 


 **At least Two Modalities (Out of Audio, Image, and Text)**
 1. Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners [Paper](https://arxiv.org/abs/2402.17723)
 2. 
