# Awesome-Any-to-Any-Generation
This repository is dedicated to showcasing state-of-the-art techniques and implementations of any-to-any generative models, which are capable of generating any modality (audio/speech, image/vision, text) from any modality (audio/speech, image/vision, text) as input.

**Models**
1. AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling [Paper](https://arxiv.org/abs/2402.12226)
2. CoDI: Any-to-Any Generation via Composable Diffusion [Paper](https://arxiv.org/abs/2305.11846)
3. CoDi2: In-Context, Interleaved, and Interactive Any-to-Any Generation [Paper](https://arxiv.org/abs/2311.18775)
4. 4M: Massively Multimodal Masked Modeling [Paper](https://arxiv.org/abs/2312.06647)
5. 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities [Paper](https://arxiv.org/abs/2406.09406)
6. C3Net: Compound Conditioned ControlNet for Multimodal Content Generation [Paper](
7. C3LLM: Conditional Multimodal Content Generation Using Large Language Models [Paper](https://arxiv.org/abs/2405.16136)
8. Visual Echoes: A Simple Unified Transformer for Audio-Visual Generation [Paper](https://arxiv.org/abs/2405.14598)
9. NExT-GPT: Any-to-Any Multimodal LLM [Paper](https://arxiv.org/abs/2309.05519)
10. Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks [Paper](https://arxiv.org/abs/2206.08916)
11. Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action [Paper](https://arxiv.org/abs/2312.17172)
13. UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models [Paper](https://arxiv.org/abs/2311.11255)
14. MM-LLMs: Recent Advances in MultiModal Large Language Models [Paper](https://arxiv.org/abs/2401.13601)
15. ModaVerse: Efficiently Transforming Modalities with LLMs [Paper](https://arxiv.org/abs/2401.06395)
16. Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy [Paper](https://arxiv.org/abs/2401.00430)
17. Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers[Paper](https://arxiv.org/abs/2405.05945)
18. WorldGPT: Empowering LLM as Multimodal World Model [Paper](https://arxiv.org/abs/2404.18202)
19. 


 **At least Two Modalities (Out of Audio, Image, and Text)**
 1. Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners [Paper](https://arxiv.org/abs/2402.17723)
 2. MULTI-MODAL LATENT DIFFUSION [Paper][https://arxiv.org/abs/2306.04445)
 3. EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs [Paper](https://arxiv.org/pdf/2310.08949)
 4. Generative Visual Instruction Tuning
 5. Multimodal Foundation Models: From Specialists to General-Purpose Assistants [Paper](https://arxiv.org/abs/2309.10020)
 6. Generating Images with Multimodal Language Models [Paper](https://arxiv.org/abs/2305.17216)
 7. 

**Survey Papers**
1. LLMs Meet Multimodal Generation and Editing: A Survey [Paper](https://arxiv.org/abs/2405.19334)
